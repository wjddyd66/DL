{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#미분 - Parameter: f(함수),x(input_value)\n",
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        \n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1-fxh2) /(2*h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GradientDescent\n",
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x=init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x = x-lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#GradientDescent 예시\n",
    "#함수선언\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "#그리기 위한 Gradient_Descent\n",
    "#x_history를 통하여 값의 변화를 저장\n",
    "def plot_gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = plot_gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "#학습률에 대한 예시\n",
    "#1. 학습률이 매우 큰 예(발산)\n",
    "init_x = np.array([-3.0,4.0])\n",
    "print(gradient_descent(function_2,init_x = init_x, lr=10.0, step_num=100)) #[-2.58983747e+13 -1.29524862e+12]\n",
    "#2. 학습률이 매우 작은 예(수렴X)\n",
    "init_x = np.array([-3.0,4.0])\n",
    "print(gradient_descent(function_2,init_x = init_x, lr=1e-10, step_num=100)) #[-2.99999994  3.99999992]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2층 Layer 선언\n",
    "class TwoLayerNet:\n",
    "\n",
    "    #Parameter 초기화\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    #예측값\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def cross_entropy_error(y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        \n",
    "        # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "             \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ca28e04caec3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# 기울기 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m#grad = network.numerical_gradient(x_batch, t_batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# 매개변수 갱신\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-220229997f63>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0ma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'softmax' is not defined"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
